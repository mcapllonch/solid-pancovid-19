{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COVID-19 Open Research Dataset Challenge - What do we know about vaccines and therapuetics?\n",
    "The following questions were analysed specifically: \n",
    "- Effectiveness of drugs being developed and tried to treat COVID-19 patients.\n",
    "  - Clinical and bench trials to investigate less common viral inhibitors against COVID-19 such as naproxen, clarithromycin, and minocyclinethat that may exert effects on viral replication.\n",
    "- Methods evaluating potential complication of Antibody-Dependent Enhancement (ADE) in vaccine recipients.\n",
    "- Exploration of use of best animal models and their predictive value for a human vaccine.\n",
    "- Capabilities to discover a therapeutic (not vaccine) for the disease, and clinical effectiveness studies to discover therapeutics, to include antiviral agents.\n",
    "- Efforts targeted at a universal coronavirus vaccine.\n",
    "- Efforts to develop animal models and standardize challenge studies\n",
    "- Assays to evaluate vaccine immune response and process development for vaccines, alongside suitable animal models (in conjunction with therapeutics)\n",
    "\n",
    "## Our approach - Creating a timeline visualizing the progress of vaccines/cures on COVID-19 and other similar viral diseases.\n",
    "Our goal is to create an intuitive visualization of the progress of research on vaccines and therapuetics regarding COVID-19. Not only is this useful for professional researchers in having a quick overview of the clinical trial stages of each investigated vaccine/therapeutic, but also for the public, to have a better understanding of the time frame for which to expect a cure or solution. We decided to create vizualizations of research progress of other virusses as well as COVID-19, to get a better picture of the timescale and ammount of research that goes into making a vaccine or therapeutics.\n",
    "\n",
    "Several steps were taken to create the visualizations:\n",
    "1. Load and preprocess the data:\n",
    "    - lemmatize all texts and remove stopwords\n",
    "2. Categorize papers based on keywords \n",
    "    - using either string pattern matching or word embeddings\n",
    "    - relevant words were manually selected based on the research questions and indicativaty of clinical stage trial (e.g. mouse vs human test subject, words expressing certainty etc.)\n",
    "    - categories are: virus, clinical stage, drug type\n",
    "3. Extract keywords/summaries from selected papers\n",
    "    - TODO: write how we do this @Simon, @Silvan\n",
    "5. Visualize extracted papers, links and summaries\n",
    "    - TODO: explain how (after we know how) @Levi @Gloria\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.a Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# TODO: write your imports here\n",
    "import os\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import pickle as pk\n",
    "import numpy as np\n",
    "\n",
    "#required additional nltk dependency\n",
    "nltk.download('punkt')\n",
    "\n",
    "# path to data\n",
    "data_dir = '../../src'  \n",
    "keyword_dir = '../../keywords'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.b Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# As kaggle only allows notebook submissions, all functions should be in the notebook. Just copy your functions and paste them here.\n",
    "          \n",
    "def load_data(data_dir):\n",
    "    \"\"\"Load data from dataset data directory.\"\"\"\n",
    "    sha = []\n",
    "    full_text = []\n",
    "\n",
    "    subdir = [x for x in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir,x))]\n",
    "\n",
    "    print(f\"[INFO] Loading data from {data_dir}...\")\n",
    "    # loop through folders with json files\n",
    "    for folder in tqdm(subdir, desc='reading folder'):\n",
    "        \n",
    "#             path = os.path.join(data_dir,folder, folder)\n",
    "        path = os.path.join(data_dir,folder, folder, 'pdf_json')\n",
    "        # loop through json files and scrape data\n",
    "        try:\n",
    "            for file in tqdm(os.listdir(path), desc='reading files'):\n",
    "                file_path = os.path.join(path, file)\n",
    "\n",
    "                # open file only if it is a file\n",
    "                if os.path.isfile(file_path):\n",
    "                    with open(file_path) as f:\n",
    "                        data_json = json.load(f)\n",
    "                        sha.append(data_json['paper_id'])\n",
    "\n",
    "                        # combine abstract texts / process\n",
    "                        combined_str = ''\n",
    "                        for text in data_json['body_text']:\n",
    "                            combined_str += text['text'].lower()\n",
    "\n",
    "                        full_text.append(combined_str)\n",
    "\n",
    "                else:\n",
    "                    print('[WARNING]', file_path, 'not a file. Check pointed path directory in load_data().')\n",
    "        except:\n",
    "            print('[WARNING]', path, 'does not exist or something else is wrong lol')\n",
    "\n",
    "    loaded_samples = len(sha)\n",
    "    print(f\"[INFO] Data loaded into dataset instance. {loaded_samples} samples added.\")\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df['sha'] = sha\n",
    "    df['full_text'] = full_text\n",
    "    \n",
    "    return df\n",
    "\n",
    "def clean_time(val):\n",
    "    try:\n",
    "        return datetime.strptime(val, '%Y-%m-%d')\n",
    "    except:\n",
    "        try:\n",
    "            return datetime.strptime(val, '%Y %b %d')\n",
    "        except:\n",
    "            try:\n",
    "                return datetime.strptime(val, '%Y %b')\n",
    "            except:\n",
    "                try:\n",
    "                    return datetime.strptime(val, '%Y')\n",
    "                except:\n",
    "                    try:\n",
    "                        return datetime.strptime('-'.join(val.split(' ')[:3]), '%Y-%b-%d')\n",
    "                    except Exception as e:\n",
    "                        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def tokenize_check(text):\n",
    "    if isinstance(text, str):\n",
    "        word_tokens = word_tokenize(text)\n",
    "    elif isinstance(text, list):\n",
    "        word_tokens = text\n",
    "    else:\n",
    "        raise TypeError\n",
    "    return word_tokens\n",
    "    \n",
    "\n",
    "def remove_stopwords(text, remove_symbols=False):\n",
    "    \"\"\" Tokenize and/or remove stopwords and/or unwanted symbols from string\"\"\"\n",
    "    list_stopwords = set(stopwords.words('english'))\n",
    "    # list of signs to be removed if parameter remove_symbols set to True\n",
    "    list_symbols = ['.', ',', '(', ')', '[', ']']\n",
    "    \n",
    "    # check input type and tokenize if not already\n",
    "    word_tokens = tokenize_check(text)\n",
    "\n",
    "    # filter out stopwords\n",
    "    text_without_stopwords = [w for w in word_tokens if not w in list_stopwords] \n",
    "    \n",
    "    if remove_symbols is True:\n",
    "        text_without_stopwords = [w for w in text_without_stopwords if not w in list_symbols]\n",
    "    \n",
    "    return text_without_stopwords\n",
    "\n",
    "# from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "def lemmatize(text):\n",
    "    \"\"\" Tokenize and/or lemmatize string \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # check input type and tokenize if not already\n",
    "    word_tokens = tokenize_check(text)\n",
    "    \n",
    "    lemmatized_text = [lemmatizer.lemmatize(w) for w in word_tokens]\n",
    "    \n",
    "    return lemmatized_text\n",
    "\n",
    "def find_keywords(text, df):\n",
    "    \"\"\" Find relevant papers for the categories in df\n",
    "    Returns a dictionary with the paper id's that match the categories\n",
    "    It also stores the sentences where the matches have been found. This can be returned too if so the team decides \"\"\"\n",
    "\n",
    "    # Data cleaning:\n",
    "    # Turn df into a dictionary with a list of key phrases\n",
    "    # Lower all of them and remove null values\n",
    "    dfd = {k: [x.lower() for x in v if not pd.isnull(x)] for k, v in df.to_dict('list').items()}\n",
    "    \n",
    "    matches = {}\n",
    "    scores = {}\n",
    "    \n",
    "    # Remove redundant values (i.e., ['coronavirus', 'coronavirus disease'] can be left as ['coronavirus']; the element 'coronavirus disease' is useless)\n",
    "    for k, v in dfd.items():\n",
    "        # print(k)\n",
    "        v = [x for x in v if not any([y in x for y in [z for z in v if z != x]])]\n",
    "        dfd[k] = v\n",
    "\n",
    "        # Find matches\n",
    "        # Use the loop we're in where we've already cleaned the data to find the matches\n",
    "        \n",
    "        # if you use keyprhase, it handles phase i and phase ii the same way, it would count both..\n",
    "        \n",
    "        for sentence in sent_tokenize(text):\n",
    "            for keyphrase in v:\n",
    "                if keyphrase in sentence:\n",
    "                    try:\n",
    "                        already_a_match = sentence in matches[k]\n",
    "                    except KeyError:\n",
    "                        matches[k] = [sentence]\n",
    "                    else:\n",
    "                        if not already_a_match:\n",
    "                            matches[k].append(sentence)\n",
    "                            \n",
    "        # score is scaled by the number of values to choose from\n",
    "        if k in matches:\n",
    "            scores[k] = len(matches)/len(v)\n",
    "\n",
    "    # return the keys with the highest score. also return the sentences for this.\n",
    "    if len(scores.keys()) > 0:\n",
    "        max_score = list(scores.keys())[np.argmax(scores.values())]\n",
    "        return max_score, matches[max_score]\n",
    "    else:\n",
    "        return 'nan','nan'\n",
    "\n",
    "def summarize(text):\n",
    "    # TODO @Simon @Silvan: extract keywords\n",
    "    return 'summary'\n",
    "\n",
    "#def visualize_data(data,keywords,summaries):\n",
    "#    #TODO @Levi @Kwan: visualize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.c Relevant strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords that define the virus the paper is about (likely in title)\n",
    "virus_keywords = pd.read_csv(keyword_dir+'/virus_keywords.csv')\n",
    "\n",
    "# keywords describing clinical phase\n",
    "clinical_stage_keywords = pd.read_csv(keyword_dir+'/phase_keywords.csv')\n",
    "\n",
    "# keywords describing treatment types\n",
    "drug_keywords = pd.read_csv(keyword_dir+'/drug_keywords.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try the preloaded dataframe to speed up the process\n",
    "try:\n",
    "    df = pk.load(open('df.pkl','rb'))\n",
    "except:\n",
    "    # create dataset object\n",
    "    meta_data = pd.read_csv(data_dir+'/metadata.csv')\n",
    "    meta_data['publish_time'] = meta_data['publish_time'].apply(clean_time)\n",
    "    full_texts = load_data(data_dir)\n",
    "\n",
    "    # merge full text and metadata, so the paper selection can be performed either on full text\n",
    "    # or abstract, if the full text is not available.\n",
    "    df = pd.merge(meta_data,full_texts,on='sha',how='outer')\n",
    "    df['full_text'][df['full_text'].isna()] = df['abstract'][df['full_text'].isna()]\n",
    "\n",
    "    # drop papers with no abstract and no full text\n",
    "    df = df.dropna(subset=['abstract','full_text'])\n",
    "    df = df[df['full_text'] != 'Unknown']\n",
    "    pk.dump(df,open('df.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define virus type, clinical stage and drug type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pk.load(open('df_kw.pkl','rb'))\n",
    "except:\n",
    "    # function on full text --> think about applying on full text or on abstract\n",
    "    df['virus'], df['virus_sentence'] = zip(*df['abstract'].apply(find_keywords,df=virus_keywords))\n",
    "    df['stage'], df['stage_sentence'] = zip(*df['abstract'].apply(find_keywords,df=clinical_stage_keywords))\n",
    "    df['drug'], df['drug_sentence'] = zip(*df['abstract'].apply(find_keywords,df=drug_keywords))\n",
    "    \n",
    "    # drop papers with nan values?\n",
    "    pk.dump(df,open('df_kw.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Summarize the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['summary'] = df['full_text'].apply(summarize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Visualize extracted papers, links and summaries\n",
    "\n",
    "The next cell will first do some extra data cleaning\n",
    "The final cell will output a web app, with a clickable web endpoint that you can use to display the cell content in full screen in the browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add some columns to the data\n",
    "df['publish_time_month'] = df.publish_time.apply(lambda x: x.strftime('%Y-%m') if not pd.isna(x) else np.nan)\n",
    "df = df.set_index('title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupyter_plotly_dash import JupyterDash\n",
    "\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import traceback\n",
    "\n",
    "'''\n",
    "This is a prototype webapp for the CORD19 challenge on Kaggle\n",
    "This involves a demo pandas dataframe, and sample visualisations\n",
    "All data here is fictional!\n",
    "'''\n",
    "\n",
    "external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\n",
    "def make_bubbleplot(df):\n",
    "    return px.scatter(\\\n",
    "                      pd.pivot_table(df.reset_index(), values=['title'], index=['virus', 'stage', 'publish_time_month'], aggfunc=np.count_nonzero).reset_index().rename(columns={'title':'count'}),\\\n",
    "                      x=\"publish_time_month\", y=\"stage\", color=\"virus\", size='count',\n",
    "                 hover_name=\"virus\", title='Occurance of research tag per month per phase sized by #Occurances')\n",
    "\n",
    "app = JupyterDash(__name__)\n",
    "\n",
    "app.layout = html.Div(children=[\n",
    "    html.H1(children='COVID-19: Visual Research Exploration Tool'),\n",
    "    html.Marquee('The data in this tool is fictional!', draggable='true'),\n",
    "    dcc.Tabs([\n",
    "        dcc.Tab(label='Overview', children=[    \n",
    "            dcc.Graph(\n",
    "            id='stage-plot',\n",
    "            figure=make_bubbleplot(df)\n",
    "    )]),\n",
    "        dcc.Tab(label='Discover', children=[\n",
    "            html.Div('virus filter'),\n",
    "            dcc.Dropdown(\n",
    "                id=f'dropdown-virus',\n",
    "                options=[{'label': k, 'value':k} for k in df.virus.unique() if not pd.isna(k)],\n",
    "                multi=True,\n",
    "                value=[k for k in df.virus.unique()]\n",
    "            ),\n",
    "            html.Div('stage filter'),\n",
    "            dcc.Dropdown(\n",
    "                id=f'dropdown-stage',\n",
    "                options=[{'label': k, 'value':k} for k in df.stage.unique()],\n",
    "                multi=True,\n",
    "                value=[k for k in df.stage.unique()]\n",
    "            ),\n",
    "            html.Div('drug filter'),\n",
    "            dcc.Dropdown(\n",
    "                id=f'dropdown-drug',\n",
    "                options=[{'label': k, 'value':k} for k in df.drug.unique()],\n",
    "                multi=True,\n",
    "                value=[k for k in df.drug.unique()]\n",
    "            ),\n",
    "            html.Div('x-axis'),\n",
    "            dcc.Dropdown(\n",
    "                id='x-axis',\n",
    "                options=[{'label': k, 'value':k} for k in ['stage', 'virus', 'drug']],\n",
    "                value='stage'\n",
    "            ),\n",
    "            html.Div('hue (color)'),\n",
    "            dcc.Dropdown(\n",
    "                id='hue-axis',\n",
    "                options=[{'label': k, 'value':k} for k in ['stage', 'virus', 'drug']],\n",
    "                value='virus'\n",
    "            ),\n",
    "            # ADD FILTER BASED ON VIRUS TYPE\n",
    "            dcc.DatePickerRange(\n",
    "                id='date-range',\n",
    "                min_date_allowed=min(df.publish_time),\n",
    "                max_date_allowed=max(df.publish_time),\n",
    "                initial_visible_month=datetime(2020, 1, 1),\n",
    "                start_date=datetime(2020, 1, 1),\n",
    "                end_date = datetime(2020, 1, 31)\n",
    "        ),\n",
    "            dcc.Graph(\n",
    "            id='discover-plot',\n",
    "            figure=None,\n",
    "        ),\n",
    "            html.P(id='selected-element')\n",
    "        ]\n",
    "        )\n",
    "    ]),\n",
    "])\n",
    "\n",
    "@app.callback(\n",
    "    Output('selected-element', 'children'),\n",
    "    [Input('discover-plot', 'clickData')]\n",
    "    )\n",
    "def show_point_data(data_dict):\n",
    "    print(data_dict)\n",
    "    title = data_dict['points'][0]['customdata'][0]\n",
    "    abstract = df.loc[title]['abstract']\n",
    "    summary = df.loc[title]['summary']\n",
    "    return [f'SUMMARY {summary}',html.Br(),html.Br(), f'ABSTRACT:{abstract}']\n",
    "\n",
    "@app.callback(\n",
    "    Output('discover-plot', 'figure'),\n",
    "    [Input('dropdown-virus', 'value'),\n",
    "    Input('dropdown-stage', 'value'),\n",
    "    Input('dropdown-drug', 'value'),\n",
    "    Input('date-range', 'start_date'),\n",
    "    Input('date-range', 'end_date'),\n",
    "    Input('x-axis', 'value'),\n",
    "    Input('hue-axis', 'value')\n",
    "    ]\n",
    "    )\n",
    "def discover_plot(virus, stage, drug, start, end, x_ax, hue_ax):\n",
    "    start = datetime.strptime(start.split('T')[0], '%Y-%m-%d')\n",
    "    end = datetime.strptime(end.split('T')[0], '%Y-%m-%d')\n",
    "    data = df[(df['publish_time'] >= start) & (df['publish_time']<= end)].copy(deep=True)\n",
    "    data = data[(data.virus.isin(virus)) & (data.stage.isin(stage)) & (data.drug.isin(drug))]\n",
    "    df['count'] = 1\n",
    "    data= data.reset_index()\n",
    "    fig = px.bar(data, x=x_ax, y='count', color=hue_ax, hover_data=['title', 'publish_time_month', 'virus', 'drug', 'stage'])\n",
    "    return fig\n",
    "\n",
    "app"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
